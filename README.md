# Tales-of-two-Minds

**Evaluating Creativity in Human and Large Language Model Narratives**

This repository contains all code and instructions for the experiments carried out by Roberto Passaro, Marta Pavanati, Clotilde Frapiccini, and Anuoluwapo Aremu for an MSc course at CIMeC, University of Trento, 2025.

We compare 21 human‑written short stories against 21 GPT‑4.1 continuations (×7 temperatures) using four automated creativity metrics:

* **Novelty**
* **Surprise**
* **Lexical Diversity**
* **Semantic Diversity**

## Introduction

Creativity has long been regarded as an exclusively human capacity, but the emergence of large language models (LLMs) raises the question of whether these models can achieve human-comparable levels of creativity in generating narratives.  In this study, we compared 21 short stories generated by ChatGPT 4.1 with 21 human-written counterparts, measuring creativity using four automatic metrics: novelty, surprise, lexical diversity, and semantic diversity. We also investigated the impact of temperature settings - a randomness control hyperparameter often associated with creative variation - on narrative creativity. We observed that novelty exhibits temperature-dependent trends, occasionally approaching human levels, and that GPT-4.1 consistently outperforms human authors in lexical diversity and surprise, while semantic diversity is consistently influenced by temperature and remains lower than that of humans. Building on these findings, we conclude by reasoning about the factors that drive the observed differences in narrative creation between humans and LLMs.

## Requirements

* Python >= 3.8
* Install dependencies via `pip install -r requirements.txt`
* SpaCy English model (once):

  ```bash
  python -m spacy download en_core_web_sm
  ```

## Installation

```bash
git clone https://github.com/youruser/tales-of-two-minds.git
cd tales-of-two-minds
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

## Data

The code expects **21 human-written stories** in `human_texts/` as UTF-8 `.txt` files named `story_01.txt`, `story_02.txt`, …, `story_21.txt`. We have **not** committed the full set here. To rerun the experiment and obtain the human dataset, please email `roberto.passaro@studenti.unitn.it`.

## Usage

Set your OpenAI key and run:

```bash
export OPENAI_API_KEY="sk-…"
python code/run_experiments.py
```

This will:

1. Generate 21 GPT-4.1 continuations at each of seven temperature settings.
2. Preprocess all texts (lemmatization, filtering).
3. Compute novelty, surprise, lexical diversity, semantic diversity, and save `scores.csv`.
4. Produce summary stats and plots in `statplots/`.

## Metrics

Below are the definitions and formulas for the four creativity metrics used:

* **Lexical Diversity (Type–Token Ratio)**

  ```text
  TTR = U / N
  ```

  * U = number of unique tokens after preprocessing
  * N = total number of tokens

* **Semantic Diversity (Inverse Homogeneity)**
  Let `sim_{ij}` be the cosine similarity between documents i and j. Then for each document i:

  ```text
  SemDiv_i = 1 - (1/(n-1)) * sum_{j != i} sim_{ij}
  ```

* **Novelty**
  Let `d_{ij}` be the cosine distance between document embeddings of i and j. Define:

  ```text
  D_local_i = (1/(n-1)) * sum_{j != i} d_{ij}
  D_global  = (1/(n*(n-1))) * sum_{i != j} d_{ij}
  ```

  Then:

  ```text
  Novelty_i = 2 * abs(D_local_i - D_global)
  ```

* **Surprise**
  For a document with m sentences, let `d_k` be the cosine distance between sentence k and k+1. Then:

  ```text
  Surprise = (2/(m-1)) * sum_{k=1 to m-1} d_k
  ```

## Results

* **scores.csv**: per-story metrics
* **statplots/**: boxplots, QQ-plots, correlation heatmaps, p-value tables

## Contributing

1. Fork the repo
2. Create a feature branch
3. Open a Pull Request

## License

This project is released under the [MIT License](LICENSE.md).

## Contact

Roberto Passaro – `<roberto.passaro@studenti.unitn.it>`


