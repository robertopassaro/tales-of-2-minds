{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPscQz9fCS8uons5N/7L2rE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertopassaro/tales-of-2-minds/blob/main/tale_of_2_minds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0ZoiMiFVbCv"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# IMPORT LIBRARIES\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# COLLECTING THE TEXTS\n",
        "!pip install sentence_transformers\n",
        "\n",
        "\n",
        "# PREPROCESSING EXISTING TEXT FILES\n",
        "import spacy\n",
        "\n",
        "# METRICS\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from nltk import word_tokenize\n",
        "\n",
        "#STATS\n",
        "from scipy.stats import shapiro, levene, probplot\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "from scipy.stats import mannwhitneyu\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LET'S CALL CHATGPT\n",
        "\n",
        "import openai\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# DIRECTORIES\n",
        "# ----------------------------------------------------------------------------\n",
        "GPT4_DIR = \"gpt4.1\"\n",
        "HUMAN_DIR = \"human_texts\"\n",
        "\n",
        "# ensure both directories are in place\n",
        "os.makedirs(GPT4_DIR, exist_ok=True)\n",
        "if not os.path.isdir(HUMAN_DIR):\n",
        "    raise FileNotFoundError(f\"Expected folder '{HUMAN_DIR}' of human stories not found\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# SETTINGS\n",
        "# ----------------------------------------------------------------------------\n",
        "# Directly embed the API key for Colab usage\n",
        "API_KEY = None #insert your API key\n",
        "\n",
        "MODEL = \"gpt-4.1\"\n",
        "N_COMPLETIONS = 21\n",
        "# Corrected temperature values as provided\n",
        "TEMPERATURES = [0.001, 0.334, 0.667, 1.0, 1.334, 1.667, 2.0]\n",
        "\n",
        "SYSTEM_INSTRUCTION = \"\"\"\n",
        "Please read the following prompt carefully and adopt the perspective of a writer.\n",
        "Your task is to continue the story in any direction you choose.\n",
        "The story should be longer than 200 words.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TEXT = \"\"\"\n",
        "Cameron, after that very stressful day, fell asleep in her bed as usual.\n",
        "The next day she opened her eyes and, to her great surprise, found herself...\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "openai.api_key = API_KEY\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# GENERATE & SAVE GPT-4.1 COMPLETIONS AT DIFFERENT TEMPERATURES\n",
        "# ----------------------------------------------------------------------------\n",
        "for temperature in TEMPERATURES:\n",
        "    temp_str = str(temperature).replace('.', '_')\n",
        "    out_dir = os.path.join(GPT4_DIR, f\"temp_{temp_str}\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    completions = []\n",
        "    while len(completions) < N_COMPLETIONS:\n",
        "        batch_size = min(10, N_COMPLETIONS - len(completions))\n",
        "        # Use new OpenAI Python v1 interface for chat completions\n",
        "        response = openai.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\",  \"content\": SYSTEM_INSTRUCTION},\n",
        "                {\"role\": \"user\",    \"content\": PROMPT_TEXT},\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            top_p=0.95,\n",
        "            n=batch_size,\n",
        "        )\n",
        "        completions.extend(\n",
        "            choice.message.content.strip()\n",
        "            for choice in response.choices\n",
        "        )\n",
        "\n",
        "    for idx, text in enumerate(completions, start=1):\n",
        "        filename = f\"gpt4_{idx}.txt\"\n",
        "        path = os.path.join(out_dir, filename)\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "        print(f\"Generated Temp={temperature} {idx}/{N_COMPLETIONS}: {path}\")"
      ],
      "metadata": {
        "id": "zXr5PGl3VxV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# PREPROCESSING EXISTING TEXT FILES – Cleaning for gpt temperature folders AND Human stories\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Load spaCy model (do this once)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Preprocessing function (define here so it’s in scope)\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Lemmatize, lowercase, and filter tokens by POS (NOUN, VERB, ADJ, ADV),\n",
        "    removing stop words and non-alphabetic tokens.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if token.pos_ in {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
        "        and not token.is_stop\n",
        "        and token.is_alpha\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Root directories\n",
        "GPT_DIR = \"gpt4.1\"\n",
        "HUMAN_DIR   = \"human_texts\"\n",
        "\n",
        "# Helper to process a given raw directory\n",
        "\n",
        "def clean_directory(raw_dir: str):\n",
        "    clean_dir = os.path.join(raw_dir, \"preprocessed\")\n",
        "    os.makedirs(clean_dir, exist_ok=True)\n",
        "    print(f\"\\n Cleaning texts in '{raw_dir}' → '{clean_dir}'\")\n",
        "    for filename in sorted(os.listdir(raw_dir)):\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            continue\n",
        "        raw_path = os.path.join(raw_dir, filename)\n",
        "        with open(raw_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().strip()\n",
        "        cleaned = preprocess_text(text)\n",
        "        clean_name = filename.replace(\".txt\", \"_clean.txt\")\n",
        "        clean_path = os.path.join(clean_dir, clean_name)\n",
        "        with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(cleaned)\n",
        "        print(f\"  • {filename} → {clean_name}\")\n",
        "\n",
        "# Process all temperature subfolders under GPT_DIR\n",
        "for temp_folder in sorted(os.listdir(GPT_DIR)):\n",
        "    raw_temp_dir = os.path.join(GPT_DIR, temp_folder)\n",
        "    if os.path.isdir(raw_temp_dir):\n",
        "        clean_directory(raw_temp_dir)\n",
        "\n",
        "# Process humaan_texts folder\n",
        "if os.path.isdir(HUMAN_DIR):\n",
        "    clean_directory(HUMAN_DIR)\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Expected folder '{HUMAN_DIR}' not found\")\n",
        "\n",
        "print(\"\\n Cleaning complete for all directories.\")"
      ],
      "metadata": {
        "id": "8C8nM1lrV0iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# LEXICAL DIVERSITY - configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "# ensure the tokenizer data is installed\n",
        "for pkg in (\"punkt\", \"punkt_tab\"):\n",
        "    try:\n",
        "        nltk.data.find(f\"tokenizers/{pkg}\")\n",
        "    except LookupError:\n",
        "        nltk.download(pkg, quiet=True)\n",
        "\n",
        "# Dynamically locate preprocessed directories and extract temperature\n",
        "PREPROC_DIRS = []\n",
        "# Base directory for GPT-4.1 outputs\n",
        "GPT_DIR = \"gpt4.1\"\n",
        "# All temperature-specific preprocessed folders under GPT_DIR\n",
        "for sub in sorted(os.listdir(GPT_DIR)):\n",
        "    pre_dir = os.path.join(GPT_DIR, sub, \"preprocessed\")\n",
        "    if os.path.isdir(prehuman_texts    # extract temperature string, e.g. sub = 'temp_0_001'\n",
        "        temp_label = sub.replace('temp_', '').replace('_', '.')\n",
        "        PREPROC_DIRS.append((pre_dir, \"gpt-4.1\", temp_label))\n",
        "# Add human preprocessed\n",
        "human_pre = os.path.join(\"human_texts\", \"preprocessed\")\n",
        "if os.path.isdir(human_human_textsREPROC_DIRS.append((human_pre, \"human\", \"human\"))\n",
        "\n",
        "OUTPUT_SCORES   = \"scores.csv\"\n",
        "OUTPUT_SUMMARY = \"lexical_diversity_summary.csv\"\n",
        "\n",
        "# lexical_diversity function\n",
        "def lexical_diversity(text: str, n: int = 1) -> float:\n",
        "    tokens = word_tokenizehuman_texts())\n",
        "    if len(tokens) < n:\n",
        " human_textsrn 0.0\n",
        "    ngrams_list = list(ngrams(tokens, n))\n",
        "    return len(sethuman_textst)) / len(ngrams_list)\n",
        "\n",
        "# Compute lexical diversity for all files\n",
        "results = []\n",
        "for preproc_dir, source_label, temp_label in PREPROC_DIRS:\n",
        "    # match all .txt files in the preprocessed folder\n",
        "    pattern = os.path.join(preproc_dir, \"*.txt\")\n",
        "    for path in sorted(glob.glob(pattern)):\n",
        "        text = open(path, 'r', encoding='utf-8').read().strip()\n",
        "        score = lexical_diversity(text, n=1)\n",
        "        results.append({\n",
        "            \"source\":            source_label,\n",
        "            \"temperature\":       temp_label,\n",
        "            \"filename\":          os.path.basename(path),\n",
        "            \"lexical_diversity\": score,\n",
        "        })\n",
        "\n",
        "if not results:\n",
        "    raise RuntimeError(\"No preprocessed files found. Check directory structure.\")\n",
        "\n",
        "# Create DataFrame of results\n",
        "scores_df = pd.DataFrame(results)\n",
        "\n",
        "# Print per-file scores\n",
        "def format_label(row):\n",
        "    return f\"{row['source']}/temp={row['temperature']}/{row['filename']}\"\n",
        "\n",
        "for _, row in scores_df.iterrows():\n",
        "    print(f\"{format_label(row)}: lexical_diversity = {row['lexical_diversity']:.4f}\")\n",
        "\n",
        "# Save full per-file scores without raw text or prompt\n",
        "scores_df.to_csv(\n",
        "    OUTPUT_SCORES,\n",
        "    index=False,\n",
        "    columns=[\"source\", \"temperature\", \"filename\", \"lexical_diversity\"]\n",
        ")\n",
        "print(f\"\\nSaved per-file scores to {OUTPUT_SCORES}\")\n",
        "\n",
        "# Grouped summary (mean TTR by source & temperature)\n",
        "summary = (\n",
        "    scores_df\n",
        "    .groupby([\"source\", \"temperature\"])[\"lexical_diversity\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "summary.to_csv(OUTPUT_SUMMARY, index=False)\n",
        "print(f\"Saved summary to {OUTPUT_SUMMARY}\\n\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "i7UxPsFbV3MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# SEMANTIC DIVERSITY - configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "# Inverse homogeneity: 1 − avg cosine similarity to all other docs in group\n",
        "def inv_hom(doc_texts: list[str]) -> np.ndarray:\n",
        "    n = len(doc_texts)\n",
        "    if n < 2:\n",
        "        return np.zeros(n, dtype=float)\n",
        "    X = TfidfVectorizer().fit_transform(doc_texts)\n",
        "    sim = cosine_similarity(X)\n",
        "    avg_sim = (sim.sum(axis=1) - 1.0) / (n - 1)\n",
        "    return 1.0 - avg_sim\n",
        "\n",
        "# Path to master CSV and load\n",
        "SCORES_CSV = \"scores.csv\"\n",
        "df_scores = pd.read_csv(SCORES_CSV)\n",
        "# Initialize semantic_diversity column\n",
        "df_scores['semantic_diversity'] = np.nan\n",
        "\n",
        "# Iterate each source-temperature group\n",
        "groups = df_scores[['source', 'temperature']].drop_duplicates()\n",
        "for _, row in groups.iterrows():\n",
        "    source, temp = row['source'], row['temperature']\n",
        "    # Determine preprocessed directory\n",
        "    if source == 'human':\n",
        "        pre_dir = os.path.join('human_texts', 'preprocessed')\n",
        "    else:\n",
        "        folder = f\"temp_{str(temp).replace('.', '_')}\"\n",
        "        pre_dir = os.path.join('gpt4.1', folder, 'preprocessed')\n",
        "    # Collect filenames for this group\n",
        "    mask = (df_scores['source'] == source) & (df_scores['temperature'] == temp)\n",
        "    filenames = df_scores.loc[mask, 'filename'].tolist()\n",
        "    # Load texts\n",
        "    docs = []\n",
        "    valid_files = []\n",
        "    for fname in sorted(filenames):\n",
        "        path = os.path.join(pre_dir, fname)\n",
        "        if os.path.isfile(path):\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read().strip()\n",
        "            docs.append(text)\n",
        "            valid_files.append(fname)\n",
        "    # Skip if no docs found\n",
        "    if not docs:\n",
        "        continue\n",
        "    # Compute semantic diversity\n",
        "    sem_div = inv_hom(docs)\n",
        "    # Map scores back and print\n",
        "    for fname, score in zip(valid_files, sem_div):\n",
        "        df_scores.loc[\n",
        "            (df_scores['source'] == source) &\n",
        "            (df_scores['temperature'] == temp) &\n",
        "            (df_scores['filename'] == fname),\n",
        "            'semantic_diversity'\n",
        "        ] = score\n",
        "        print(f\"{source}/temp={temp}/{fname}: semantic_diversity = {score:.4f}\")\n",
        "\n",
        "# Save updated CSV\n",
        "df_scores.to_csv(SCORES_CSV, index=False)\n",
        "print(f\"\\n Updated {SCORES_CSV} with semantic_diversity column\")"
      ],
      "metadata": {
        "id": "e1WrvX2pV5oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# NOVELTY – configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Path to your master CSV\n",
        "SCORES_CSV = \"scores.csv\"\n",
        "df_scores = pd.read_csv(SCORES_CSV)\n",
        "# Initialize novelty column\n",
        "df_scores['novelty'] = np.nan\n",
        "\n",
        "# Load embedding model once\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Determine unique source-temperature groups\n",
        "groups = df_scores[['source', 'temperature']].drop_duplicates()\n",
        "for _, row in groups.iterrows():\n",
        "    source, temp = row['source'], row['temperature']\n",
        "    # Identify corresponding preprocessed directory\n",
        "    if source == 'human':\n",
        "        pre_dir = os.path.join('human_texts', 'preprocessed')\n",
        "    else:\n",
        "        folder = f\"temp_{str(temp).replace('.', '_')}\"\n",
        "        pre_dir = os.path.join('gpt4.1', folder, 'preprocessed')\n",
        "\n",
        "    # Collect filenames for this group\n",
        "    mask = (df_scores['source'] == source) & (df_scores['temperature'] == temp)\n",
        "    filenames = df_scores.loc[mask, 'filename'].tolist()\n",
        "    if not filenames:\n",
        "        continue\n",
        "\n",
        "    # Load texts in sorted order matching filenames\n",
        "    texts = []\n",
        "    valid_files = []\n",
        "    for fname in sorted(filenames):\n",
        "        path = os.path.join(pre_dir, fname)\n",
        "        if os.path.isfile(path):\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                texts.append(f.read().strip())\n",
        "            valid_files.append(fname)\n",
        "\n",
        "    N = len(texts)\n",
        "    if N == 0:\n",
        "        continue\n",
        "\n",
        "    # Encode and compute distances\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "    dist_mat = cosine_distances(embeddings)\n",
        "    if N > 1:\n",
        "        D_local = dist_mat.sum(axis=1) / (N - 1)\n",
        "        D_global = dist_mat.sum() / (N * (N - 1))\n",
        "    else:\n",
        "        D_local = np.zeros(N)\n",
        "        D_global = 0.0\n",
        "    novelty_scores = 2 * np.abs(D_local - D_global)\n",
        "\n",
        "    # Map scores back into df_scores and print\n",
        "    for fname, score in zip(valid_files, novelty_scores):\n",
        "        df_scores.loc[\n",
        "            (df_scores['source'] == source) &\n",
        "            (df_scores['temperature'] == temp) &\n",
        "            (df_scores['filename'] == fname),\n",
        "            'novelty'\n",
        "        ] = score\n",
        "        print(f\"{source}/temp={temp}/{fname}: novelty = {score:.4f}\")\n",
        "\n",
        "# Save updated scores.csv\n",
        "df_scores.to_csv(SCORES_CSV, index=False)\n",
        "print(f\"\\n Updated {SCORES_CSV} with novelty column\")"
      ],
      "metadata": {
        "id": "nMGUahJpV8Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# SURPRISE – compute 2/(m-1)*Σ d(F_i, F_{i+1}) on RAW texts\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# 1) Prep\n",
        "nltk.download('punkt', quiet=True)\n",
        "SCORES_CSV = \"scores.csv\"\n",
        "df_scores = pd.read_csv(SCORES_CSV)\n",
        "\n",
        "# Cast temperature to string so it matches temp_label later\n",
        "df_scores['temperature'] = df_scores['temperature'].astype(str)\n",
        "\n",
        "# (Re)initialize the column\n",
        "df_scores['surprise'] = np.nan\n",
        "\n",
        "# 2) Model\n",
        "sur_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def surprise_score(text: str) -> float:\n",
        "    sents = sent_tokenize(text)\n",
        "    m = len(sents)\n",
        "    if m < 2:\n",
        "        return 0.0\n",
        "    embs = sur_model.encode(sents, convert_to_numpy=True)\n",
        "    dists = [\n",
        "        cosine_distances([embs[i]], [embs[i+1]])[0][0]\n",
        "        for i in range(m - 1)\n",
        "    ]\n",
        "    return 2 * sum(dists) / (m - 1)\n",
        "\n",
        "# 3) Gather raw‐text sources\n",
        "raw_sources = []\n",
        "for sub in sorted(os.listdir(\"gpt4.1\")):\n",
        "    raw_dir = os.path.join(\"gpt4.1\", sub)\n",
        "    if os.path.isdir(raw_dir):\n",
        "        # same string format used in df_scores['temperature']\n",
        "        temp_label = sub.replace(\"temp_\", \"\").replace(\"_\", \".\")\n",
        "        raw_sources.append((raw_dir, \"gpt-4.1\", temp_label))\n",
        "\n",
        "if os.path.isdir(\"human_texts\"):\n",
        "    raw_sources.append((\"human_texts\", \"human\", \"human\"))\n",
        "else:\n",
        "    raise FileNotFoundError(\"Expected 'human_texts' directory not found\")\n",
        "\n",
        "# 4) Compute and assign\n",
        "for raw_dir, src, temp_label in raw_sources:\n",
        "    for path in sorted(glob.glob(os.path.join(raw_dir, \"*.txt\"))):\n",
        "        text = open(path, encoding=\"utf-8\").read().strip()\n",
        "        score = surprise_score(text)\n",
        "        clean_name = os.path.basename(path).replace(\".txt\", \"_clean.txt\")\n",
        "\n",
        "        # Mask now compares strings to strings\n",
        "        mask = (\n",
        "            (df_scores[\"source\"]      == src) &\n",
        "            (df_scores[\"temperature\"] == temp_label) &\n",
        "            (df_scores[\"filename\"]    == clean_name)\n",
        "        )\n",
        "        df_scores.loc[mask, \"surprise\"] = score\n",
        "\n",
        "# 5) Save back\n",
        "df_scores.to_csv(SCORES_CSV, index=False)\n",
        "print(f\"Updated {SCORES_CSV} with surprise scores\")"
      ],
      "metadata": {
        "id": "drzZjjqXV-gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Preliminary Checks\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('scores.csv')   # ← corrected path\n",
        "\n",
        "metrics = ['lexical_diversity', 'semantic_diversity', 'novelty', 'surprise']\n",
        "sources = df['source'].unique()\n",
        "\n",
        "# 1. Descriptive Statistics\n",
        "desc = df.groupby('source')[metrics].agg(['mean', 'std']).round(4)\n",
        "print(\"Descriptive Statistics (mean ± std):\")\n",
        "print(desc)\n",
        "\n",
        "# 2. Normality Tests (Shapiro–Wilk)\n",
        "print(\"\\nShapiro–Wilk Normality Tests:\")\n",
        "for metric in metrics:\n",
        "    for source in sources:\n",
        "        data = df[df['source'] == source][metric].dropna()\n",
        "        stat, p = shapiro(data)\n",
        "        print(f\"{source} – {metric}: W={stat:.4f}, p={p:.4f}\")\n",
        "\n",
        "# 3. Homogeneity of Variance (Levene’s Test)\n",
        "print(\"\\nLevene’s Test for Equal Variances:\")\n",
        "for metric in metrics:\n",
        "    g1 = df[df['source'] == sources[0]][metric].dropna()\n",
        "    g2 = df[df['source'] == sources[1]][metric].dropna()\n",
        "    stat, p = levene(g1, g2)\n",
        "    print(f\"{metric}: W={stat:.4f}, p={p:.4f}\")\n",
        "\n",
        "# 4. Correlation Matrix\n",
        "corr = df[metrics].corr()\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(corr)\n",
        "\n",
        "# 5. Q–Q Plots for Normality by Group\n",
        "nltk.download('punkt', quiet=True)\n",
        "for metric in metrics:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    for i, source in enumerate(sources, 1):\n",
        "        plt.subplot(1, 2, i)\n",
        "        probplot(df[df['source']==source][metric].dropna(), dist=\"norm\", plot=plt)\n",
        "        plt.title(f\"Q–Q: {metric} ({source})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 6. Boxplots by Group\n",
        "for metric in metrics:\n",
        "    plt.figure()\n",
        "    data = [df[df['source']==s][metric].dropna() for s in sources]\n",
        "    plt.boxplot(data, labels=sources)\n",
        "    plt.title(f\"Boxplot of {metric}\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.show()\n",
        "\n",
        "# 7. Correlation Heatmap\n",
        "plt.figure(figsize=(5,4))\n",
        "im = plt.imshow(corr, interpolation='nearest')\n",
        "plt.colorbar(im)\n",
        "plt.xticks(np.arange(len(metrics)), metrics, rotation=45)\n",
        "plt.yticks(np.arange(len(metrics)), metrics)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EVSmzreiWBXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"scores.csv\")\n",
        "\n",
        "temperatures = 'temperature'\n",
        "metric_columns = ['lexical_diversity', 'semantic_diversity', 'novelty', 'surprise']\n",
        "sources = df['source'].unique()\n",
        "\n",
        "output_folder = \"statplots\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "#Permutation Test Function:\n",
        "def permutation_test(x, y, n_permutations=10000):\n",
        "    observed_diff = np.mean(x) - np.mean(y)\n",
        "    combined = np.concatenate([x, y])\n",
        "    count = 0\n",
        "    for _ in range(n_permutations):\n",
        "        np.random.shuffle(combined)\n",
        "        new_x = combined[:len(x)]\n",
        "        new_y = combined[len(x):]\n",
        "        diff = np.mean(new_x) - np.mean(new_y)\n",
        "        if abs(diff) >= abs(observed_diff):\n",
        "            count += 1\n",
        "    p_val = count / n_permutations\n",
        "    return observed_diff, p_val  #SO: the results we see report the observed difference between the two groups and the p-value\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for temperature in sorted(df[temperatures].dropna().unique()):\n",
        "  if temperature != 'human':\n",
        "    for metric in metric_columns:\n",
        "        human_scores = df[df['source'] == 'human'][metric].dropna().values\n",
        "        llm_scores = df[(df['source'] == 'gpt-4.1') & (df['temperature'] == temperature)][metric].dropna().values\n",
        "\n",
        "        #Mann–Whitney U Test\n",
        "        u_stat, u_p = mannwhitneyu(human_scores, llm_scores, alternative='two-sided')\n",
        "\n",
        "        #Permutation Test\n",
        "        perm_diff, perm_p = permutation_test(human_scores, llm_scores)\n",
        "\n",
        "        print(f\"\\n===== {metric.upper()} at temperature level {temperature}=====\")\n",
        "        print(f\"Mann–Whitney U: U = {u_stat:.2f}, p = {u_p:.12f}\") #the .10f defines the amount of decimal positions we want to include. I manipulated it because of the p value of the semantic sim, which is super super small.\n",
        "        print(f\"Human median: {np.median(human_scores):.3f}\")\n",
        "        print(f\"LLM median: {np.median(llm_scores):.3f}\")\n",
        "        print(f\"Permutation test: Mean diff = {perm_diff:.4f}, p = {perm_p:.12f}\")\n",
        "\n",
        "  #Plot:\n",
        "        df_plot = pd.DataFrame({\n",
        "            'source': ['Human'] * len(human_scores) + [f'LLM (T={temperature})'] * len(llm_scores),\n",
        "            'score': np.concatenate([human_scores, llm_scores])\n",
        "        })\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.boxplot(data=df_plot, x='source', y='score', palette='Set2')\n",
        "        sns.stripplot(data=df_plot, x='source', y='score', color='black', alpha=0.5)\n",
        "        plt.title(f'{metric.replace(\"_\", \" \").title()} Score\\nHuman vs LLM (T={temperature})')\n",
        "        plt.ylabel(f'{metric.replace(\"_\", \" \").title()}')\n",
        "        plt.xlabel('Agent Type')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plot_filename = os.path.join(output_folder, f\"{metric}_T{temperature}_comparison.png\")\n",
        "        plt.savefig(plot_filename)\n",
        "        plt.close()\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "# Create dictionary to store p-values for each metric\n",
        "pval_tables = {metric: [] for metric in metric_columns}\n",
        "\n",
        "for temperature in sorted(df[temperatures].dropna().unique()):\n",
        "    if temperature != 'human':\n",
        "        for metric in metric_columns:\n",
        "            human_scores = df[df['source'] == 'human'][metric].dropna().values\n",
        "            llm_scores = df[(df['source'] == 'gpt-4.1') & (df['temperature'] == temperature)][metric].dropna().values\n",
        "\n",
        "            u_stat, u_p = mannwhitneyu(human_scores, llm_scores, alternative='two-sided')\n",
        "            perm_diff, perm_p = permutation_test(human_scores, llm_scores)\n",
        "\n",
        "            pval_tables[metric].append({\n",
        "                'Temperature': f'T={temperature}',\n",
        "                'Mann–Whitney U p-value': u_p,\n",
        "                'Permutation Test p-value': perm_p\n",
        "            })\n",
        "\n",
        "# Create and save tables as plots\n",
        "for metric, rows in pval_tables.items():\n",
        "    table_df = pd.DataFrame(rows)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 0.5 + 0.4 * len(table_df)))  # dynamic height\n",
        "    ax.axis('off')\n",
        "    table = plt.table(\n",
        "        cellText=np.round(table_df.iloc[:, 1:].values, 12),\n",
        "        rowLabels=table_df['Temperature'],\n",
        "        colLabels=table_df.columns[1:],\n",
        "        loc='center',\n",
        "        cellLoc='center'\n",
        "    )\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1, 1.5)\n",
        "\n",
        "    plt.title(f'{metric.replace(\"_\", \" \").title()} – P-Values vs Human')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    table_filename = os.path.join(output_folder, f\"{metric}_pvalue_table.png\")\n",
        "    plt.savefig(table_filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "#summary plots\n",
        "for metric in metric_columns:\n",
        "    summary_data = []\n",
        "\n",
        "    for temperature in sorted(df[temperatures].dropna().unique()):\n",
        "        llm_scores = df[(df['source'] == 'gpt-4.1') & (df['temperature'] == temperature)][metric].dropna().values\n",
        "        summary_data.extend(zip([f'T={temperature}'] * len(llm_scores), llm_scores))\n",
        "\n",
        "    human_scores = df[df['source'] == 'human'][metric].dropna().values\n",
        "    summary_data.extend(zip(['Human'] * len(human_scores), human_scores))\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data, columns=['Source', 'Score'])\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.boxplot(data=summary_df, x='Source', y='Score', palette='Set2', showfliers=False)\n",
        "    sns.stripplot(data=summary_df, x='Source', y='Score', color='black', alpha=0.5, jitter=True)\n",
        "    plt.title(f'{metric.replace(\"_\", \" \").title()} Score\\nAcross Temperatures + Human')\n",
        "    plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "    plt.xlabel('Agent / Temperature')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    summary_filename = os.path.join(output_folder, f\"{metric}_summary_plot.png\")\n",
        "    plt.savefig(summary_filename)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "8AVZkk8mWGVp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}